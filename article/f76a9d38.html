<!DOCTYPE html><html><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="浅谈信安文章搜索引擎"><meta name="keywords" content=""><meta name="author" content="ye1s,undefined"><meta name="copyright" content="ye1s"><title>浅谈信安文章搜索引擎【ye1s】</title><link rel="stylesheet" href="/css/fan.css"><link rel="stylesheet" href="/css/thirdparty/jquery.mCustomScrollbar.min.css"><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"><link rel="icon" href="/favicon.ico"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.4/dist/instantsearch.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.4/dist/instantsearch-theme-algolia.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.4"></script><!-- link(rel="dns-prefetch" href="https://cdn.jsdelivr.net")--><!-- link(rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css")--><!-- script(src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer)--><!-- script(src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML")--><script src="/js/mathjax/mathjax.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
});
</script><script>var isPassword = '' || false;
if (isPassword) {
    if (prompt('请输入文章密码') !== '') {
        alert('密码错误！');
        history.back();
    }
}</script><script>window.GLOBAL_CONFIG = {
  root: '/',
  algolia: {"appId":"LKL6Q0GQJM","apiKey":"03829f64e2f5c11e4a5e2b8e51e24eb9","indexName":"blog","hits":{"per_page":10},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  gitment: {"owner":"ye1sec","repo":"comments","client_id":"d5ece338867af32b6dfa","client_secret":"2caf36bbd47977524017f95105315fc9e65f0577"},
}</script><meta name="generator" content="Hexo 4.2.1"><link rel="alternate" href="/atom.xml" title="ye1s" type="application/atom+xml">
</head><body><canvas id="universe"></canvas><!--#body--><div id="sidebar"><div class="toggle-sidebar-info button-hover"><span data-toggle="文章目录">站点概览</span></div><div class="sidebar-toc"><div class="sidebar-toc-title">目录</div><div class="sidebar-toc-progress"><span class="progress-notice">您已阅读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc-progress-bar"></div></div><div class="sidebar-toc-content" id="sidebar-toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#0x1数据获取"><span class="toc-number">1.</span> <span class="toc-text">0x1数据获取</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#0x1-1爬取站点"><span class="toc-number">1.1.</span> <span class="toc-text">0x1.1爬取站点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#0x1-2爬取方式"><span class="toc-number">1.2.</span> <span class="toc-text">0x1.2爬取方式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#0x1-2-1-网页特征"><span class="toc-number">1.2.1.</span> <span class="toc-text">0x1.2.1 网页特征</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#0x1-2-2-数据接口"><span class="toc-number">1.2.2.</span> <span class="toc-text">0x1.2.2 数据接口</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#0x1-3-反爬策略绕过"><span class="toc-number">1.3.</span> <span class="toc-text">0x1.3 反爬策略绕过</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#0x1-3-1-User-Agent"><span class="toc-number">1.3.1.</span> <span class="toc-text">0x1.3.1 User-Agent</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#0x1-3-2-IP限制"><span class="toc-number">1.3.2.</span> <span class="toc-text">0x1.3.2 IP限制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#0x1-3-3-Cookie"><span class="toc-number">1.3.3.</span> <span class="toc-text">0x1.3.3 Cookie</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#0x1-3-4-Header"><span class="toc-number">1.3.4.</span> <span class="toc-text">0x1.3.4 Header</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#0x2-数据检索"><span class="toc-number">2.</span> <span class="toc-text">0x2.数据检索</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#0x2-1-倒排索引"><span class="toc-number">2.1.</span> <span class="toc-text">0x2.1 倒排索引</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#0x2-2-elasticsearch"><span class="toc-number">2.2.</span> <span class="toc-text">0x2.2 elasticsearch</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#0x3-数据搜索"><span class="toc-number">3.</span> <span class="toc-text">0x3.数据搜索</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#0x4-系统展示"><span class="toc-number">4.</span> <span class="toc-text">0x4 系统展示</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info-avatar"><img class="author-info-avatar-img" src="/avatar.png"></div><div class="author-info-name">ye1s</div><div class="author-info-description"></div><div class="links-buttons"><a class="links-button button-hover" href="https://github.com/ye1sec" target="_blank">GitHub<i class="icon-dot bg-color5"></i></a><a class="links-button button-hover" href="mailto:431774437@qq.com" target="_blank">E-Mail<i class="icon-dot bg-color2"></i></a><a class="links-button button-hover" href="tencent://message/?uin=431774437&amp;Site=&amp;Menu=yes" target="_blank">QQ<i class="icon-dot bg-color5"></i></a></div><div class="author-info-articles"><a class="author-info-articles-archives article-meta" href="/archives"><span class="pull-top">日志</span><span class="pull-bottom">200</span></a><a class="author-info-articles-tags article-meta" href="/tags"><span class="pull-top">标签</span><span class="pull-bottom">149</span></a><a class="author-info-articles-categories article-meta" href="/categories"><span class="pull-top">分类</span><span class="pull-bottom">12</span></a></div><div class="friend-link"><a class="friend-link-text" href="http://www.m00nback.xyz/" target="_blank">MoonBack</a><a class="friend-link-text" target="_blank">待定</a></div></div></div><div id="main-container"><header><div id="menu-outer"><i class="menu-list-icon fas fa-bars"></i><nav id="menu-inner"><a class="menu-item" href="/">首页</a><a class="menu-item" href="/tags">标签</a><a class="menu-item" href="/categories">分类</a><a class="menu-item" href="/archives">归档</a><a class="menu-item" href="/about">关于</a></nav><div class="right-info"><a class="search social-icon"><i class="fas fa-search"></i><span> 搜索</span></a><a class="title-name" href="/">ye1s</a><span id="now-time"></span></div></div></header><div id="content-outer"><div id="content-inner"><article id="post"><div class="post-header"><div class="title">浅谈信安文章搜索引擎</div><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 发表于 2020-12-04 | 更新于 2020-12-27</time><!--time.button-hover.post-date #[i.fas.fa-calendar-alt.article-icon(aria-hidden="true")] #[=__('post.modified')] #[=date(page['updated'], config.date_format)]--><div class="button-hover categories"></div><div class="button-hover tags"></div></div></div><div class="main-content"><p>一个搜索引擎的实现流程大概为：首先获取海量的数据，整理成统一的格式，然后交给索引程序建立索引，当索引建立好后，就可以进行搜索。简而言之就是：数据获取<code>-&gt;</code>数据检索<code>-&gt;</code>数据搜索</p>
<a id="more"></a>

<h1 id="0x1数据获取"><a href="#0x1数据获取" class="headerlink" title="0x1数据获取"></a>0x1数据获取</h1><p>数据获取大概有如下两种：   </p>
<ul>
<li>爬虫定期获取：根据网站特征，写爬虫规则，定期获取想要的文章数据</li>
<li>网站主动推送：网站拥有者主动向搜索引擎提交文章数据</li>
</ul>
<p>搜索引擎的初期数据获取一般只能采取爬虫定期获取，当搜索引擎比较普遍使用(如谷歌、百度等)，才会有很多网站拥有者主动推送。             </p>
<h2 id="0x1-1爬取站点"><a href="#0x1-1爬取站点" class="headerlink" title="0x1.1爬取站点"></a>0x1.1爬取站点</h2><p>信息安全文章的站点，可以分为三类       </p>
<ul>
<li>安全社区：先知社区、安全客、嘶吼、freebuf、安全脉搏、91ri、看雪论坛、乌云知识库等</li>
<li>创作社区：博客园、csdn、简书、知乎、腾讯云社区等</li>
<li>个人博客：hexo主题博客、wordpress博客等  </li>
</ul>
<h2 id="0x1-2爬取方式"><a href="#0x1-2爬取方式" class="headerlink" title="0x1.2爬取方式"></a>0x1.2爬取方式</h2><p>在爬取之前，先弄清一下爬取的需求，每篇文章需要获取发布日期、作者、标题、正文内容、文章链接、网站域名。接着对文章重复的判断，这里主要是根据文章链接的唯一性来判断是否重复，当然有的文章可能会在多处发表，存在一小部分的重复文章，最后根据每个网站特点，写定制化爬虫。</p>
<p>爬取的方式可以分为两种，一种是根据网站页面特征来爬取，一种是请求数据接口来爬取。本次爬虫使用的 python 的 Scrapy 框架来演示。   </p>
<h3 id="0x1-2-1-网页特征"><a href="#0x1-2-1-网页特征" class="headerlink" title="0x1.2.1 网页特征"></a>0x1.2.1 网页特征</h3><p>通过观察 HTML 页面，先确定一下要爬取信息所在的位置，然后看一下该位置所处的 DOM 路径、标签元素、属性元素，找到能准确获取该信息的方式。以 hexo 的 next 主题博客为例，这个主题还是挺多师傅使用的。     </p>
<p>爬取思路：<br>(1). 爬取当前页面的所有文章链接<br>(2). 对页面中的每个文章链接进行爬取,得到文章相关信息<br>(3). 当前页面爬取完后，获取“下一页”,再从步骤(1)爬取</p>
<p>获取链接<br><img src="../../images/scrapy/secarticle/1.png" alt=""><br>获取文章信息<br><img src="../../images/scrapy/secarticle/2.png" alt=""><br>得到下一页面<br><img src="../../images/scrapy/secarticle/3.jpg" alt=""></p>
<p>在Scrapy 中可以使用 Scrapy shell 调试</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy shell article_url</span><br></pre></td></tr></table></figure>
<p><img src="../../images/scrapy/secarticle/4.png" alt=""></p>
<p>Scrapy中的Spider代码如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> parse</span><br><span class="line"><span class="keyword">from</span> crawlersec.items <span class="keyword">import</span> CrawlersecItem</span><br><span class="line"><span class="keyword">from</span> crawlersec.util <span class="keyword">import</span> html_entity</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SiHouSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name=<span class="string">"hexo_next"</span></span><br><span class="line">    start_urls = [<span class="string">"https://chybeta.github.io/"</span>]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        urls=response.xpath(<span class="string">"//a[@class='post-title-link']/@href"</span>).extract()<span class="comment">#得到页面的所有文章链接</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(urls)):</span><br><span class="line">            absolute_url=parse.urljoin(response.url,urls[i])</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url=absolute_url, callback=self.parse_text)</span><br><span class="line">        next_page=response.xpath(<span class="string">"//a[@class='extend next']/@href"</span>).extract_first()</span><br><span class="line">        <span class="keyword">if</span> next_page:<span class="comment">#获取下一页页面</span></span><br><span class="line">            absolute_url = parse.urljoin(response.url, next_page)</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url=absolute_url, callback=self.parse)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_text</span><span class="params">(self,response)</span>:</span><span class="comment">#获取文章的相关信息</span></span><br><span class="line">        item=CrawlersecItem()</span><br><span class="line">        item[<span class="string">'url'</span>]=response.url</span><br><span class="line">        item[<span class="string">'title'</span>]=html_entity(response.css(<span class="string">".post-title::text"</span>).extract_first().strip())</span><br><span class="line">        item[<span class="string">'author'</span>]=get_author_by_url(response.url)</span><br><span class="line">        item[<span class="string">'date'</span>]=response.xpath(<span class="string">"//time/text()"</span>).extract_first().strip()</span><br><span class="line">        content=<span class="string">""</span></span><br><span class="line">        <span class="keyword">for</span> text <span class="keyword">in</span> response.xpath(<span class="string">"//div[@class='post-body']//text()"</span>).extract():</span><br><span class="line">            content +=<span class="string">""</span>.join(text.split())</span><br><span class="line">        content=html_entity(content)</span><br><span class="line">        item[<span class="string">'content'</span>]=content</span><br><span class="line">        item[<span class="string">'domain'</span>]=list(parse.urlparse(response.url))[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">yield</span> item</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_author_by_url</span><span class="params">(url)</span>:</span></span><br><span class="line">    authors = &#123;<span class="string">"chybeta.github.io"</span>: <span class="string">"chybeta"</span>&#125;</span><br><span class="line">    <span class="keyword">return</span> authors[list(parse.urlparse(url))[<span class="number">1</span>]]</span><br></pre></td></tr></table></figure>
<h3 id="0x1-2-2-数据接口"><a href="#0x1-2-2-数据接口" class="headerlink" title="0x1.2.2 数据接口"></a>0x1.2.2 数据接口</h3><p>有些站点的文章信息是从数据接口请求而来，刚好可以直接请求数据接口获取文章的信息。例如安全客的文章：</p>
<p>安全客的网站结构是相对比较复杂，但点击加载更多，发现文章信息是通过请求数据接口得来的。<br>数据接口为</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://api.anquanke.com/data/v1/posts?size=20&amp;page=1</span><br></pre></td></tr></table></figure>
<p>爬取思路为：<br>(1).首先获取当前数据分页中每条数据文章标题、文章id、发布日期、作者<br>(2).将获取的文章id，都加上<code>https://www.anquanke.com/post/id/</code>，得到文章链接，请求文章链接获取正文内容<br>(3).获取下一数据分页，重复步骤(1)<br><img src="../../images/scrapy/secarticle/5.png" alt=""><br><img src="../../images/scrapy/secarticle/6.png" alt="">  </p>
<p>Scrapy中的Spider代码如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> parse</span><br><span class="line"><span class="keyword">import</span> simplejson</span><br><span class="line"><span class="keyword">from</span> crawlersec.items <span class="keyword">import</span> CrawlersecItem</span><br><span class="line"><span class="keyword">from</span> crawlersec.util <span class="keyword">import</span> html_entity</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AnquankeSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name=<span class="string">"anquanke"</span></span><br><span class="line">    allowed_domains=[<span class="string">"anquanke.com"</span>]</span><br><span class="line">    base_url=<span class="string">"https://www.anquanke.com/"</span></span><br><span class="line">    start_urls=[<span class="string">"https://api.anquanke.com/data/v1/posts?size=20"</span>]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self,response)</span>:</span></span><br><span class="line">        prefix_url=<span class="string">"https://www.anquanke.com/post/id/"</span></span><br><span class="line">        res=simplejson.loads(response.text)</span><br><span class="line">        posts=res[<span class="string">'data'</span>]</span><br><span class="line">        <span class="keyword">for</span> post <span class="keyword">in</span> posts:</span><br><span class="line">            item=CrawlersecItem()</span><br><span class="line">            item[<span class="string">'author'</span>]=post[<span class="string">'author'</span>][<span class="string">'nickname'</span>]</span><br><span class="line">            item[<span class="string">'date'</span>]=post[<span class="string">'date'</span>]</span><br><span class="line">            item[<span class="string">'title'</span>] = post[<span class="string">'title'</span>]</span><br><span class="line">            url=prefix_url+str(post[<span class="string">'id'</span>])</span><br><span class="line">            item[<span class="string">'url'</span>]=url</span><br><span class="line">            item[<span class="string">'domain'</span>]=list(parse.urlparse(self.base_url))[<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url=url,meta=&#123;<span class="string">'article_item'</span>:item&#125;,callback=self.parse_text)</span><br><span class="line">        next_url=res[<span class="string">'next'</span>]</span><br><span class="line">        <span class="keyword">if</span> next_url:</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url=next_url,callback=self.parse)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_text</span><span class="params">(self,response)</span>:</span></span><br><span class="line">        item=response.meta.get(<span class="string">'article_item'</span>,<span class="string">''</span>)</span><br><span class="line">        content=<span class="string">""</span></span><br><span class="line">        <span class="keyword">for</span> text <span class="keyword">in</span> response.xpath(<span class="string">"//text()"</span>).extract():</span><br><span class="line">            content +=<span class="string">""</span>.join(text.split())</span><br><span class="line">        content=html_entity(content)<span class="comment">#HTML entity encoding</span></span><br><span class="line">        item[<span class="string">'content'</span>]=content</span><br><span class="line">        print(item[<span class="string">'content'</span>])</span><br><span class="line">        <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure>
<h2 id="0x1-3-反爬策略绕过"><a href="#0x1-3-反爬策略绕过" class="headerlink" title="0x1.3 反爬策略绕过"></a>0x1.3 反爬策略绕过</h2><h3 id="0x1-3-1-User-Agent"><a href="#0x1-3-1-User-Agent" class="headerlink" title="0x1.3.1 User-Agent"></a>0x1.3.1 User-Agent</h3><p>反爬策略：网站在处理反爬的过程中，很常见的一种方式就是通过检测 User-agent 来拒绝非浏览器的访问。</p>
<p>绕过方式：可以维护一个 User-agent 组合列表，在发送请求时随机从列表中抽取一个，放入 Headers 请求头部里。   </p>
<p>可以在 Scrapy 的 middlewares.py 中自定义 RandomUserAgentMiddleware 类，并作为Download Middleware 启用。Download Middware 是引擎和下载器的中间件，每个 Request 在爬取之前都会调用其中开启的类，从而对 Request 进行一定的处理，在这里对每个请求加上随机的User-Agent</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RandomUserAgentMiddleware</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request, spider)</span>:</span></span><br><span class="line">        rand_use  = random.choice(USER_AGENT_LIST)</span><br><span class="line">        <span class="keyword">if</span> rand_use:</span><br><span class="line">            request.headers.setdefault(<span class="string">'User-Agent'</span>, rand_use)</span><br></pre></td></tr></table></figure>
<p>在 Scrapy 的 setting.py 中定义 USER_AGENT_LIST </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">USER_AGENT_LIST=[</span><br><span class="line">    <span class="string">"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1"</span>,</span><br><span class="line">    <span class="string">"Mozilla/5.0 (X11; CrOS i686 2268.111.0) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11"</span>,</span><br><span class="line">    <span class="string">"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1092.0 Safari/536.6"</span>,</span><br><span class="line">    <span class="string">"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1090.0 Safari/536.6"</span>,</span><br><span class="line">    <span class="string">"Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/19.77.34.5 Safari/537.1"</span>,</span><br><span class="line">    <span class="string">"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.9 Safari/536.5"</span>,</span><br><span class="line">    <span class="string">"Mozilla/5.0 (Windows NT 6.0) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.36 Safari/536.5"</span>,</span><br><span class="line">    <span class="string">"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3"</span>,</span><br><span class="line">    <span class="string">"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3"</span>,</span><br><span class="line">    <span class="string">"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SE 2.X MetaSr 1.0; SE 2.X MetaSr 1.0; .NET CLR 2.0.50727; SE 2.X MetaSr 1.0)"</span>,</span><br><span class="line">    <span class="string">"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3"</span>,</span><br><span class="line">    <span class="string">"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3"</span>,</span><br><span class="line">    <span class="string">"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; 360SE)"</span>,</span><br><span class="line">    <span class="string">"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3"</span>,</span><br><span class="line">    <span class="string">"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3"</span>,</span><br><span class="line">    <span class="string">"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.0 Safari/536.3"</span>,</span><br><span class="line">    <span class="string">"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24"</span>,</span><br><span class="line">    <span class="string">"Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24"</span></span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>在 setting.py 的 Download Middware 参数配置中添加定义的类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">    <span class="string">'crawlersec.middlewares.CrawlersecDownloaderMiddleware'</span>: <span class="number">543</span>,</span><br><span class="line">    <span class="string">'crawlersec.middlewares.RandomUserAgentMiddleware'</span>: <span class="number">400</span>,</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="0x1-3-2-IP限制"><a href="#0x1-3-2-IP限制" class="headerlink" title="0x1.3.2 IP限制"></a>0x1.3.2 IP限制</h3><p>反爬策略：同一 IP 访问网站过于频繁，就会对该 IP 进行限制，短时间内无法访问。<br>绕过方式：<br>1.维护一个 IP 代理池，每次请求随机使用 一个 IP 代理<br>2.调小爬虫的线程并发数，或每次请求后，设置一个短时间暂停 </p>
<p>这里就从一些免费的站点中获取一些 IP,并保存在 proxies.py 文件中<br>如 <a href="http://www.xiladaili.com" target="_blank" rel="noopener">www.xiladaili.com</a> 站点</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SeeBugSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name=<span class="string">"proxy"</span></span><br><span class="line">    allowed_domains=[<span class="string">"www.xiladaili.com"</span>]</span><br><span class="line">    base_url=<span class="string">"http://www.xiladaili.com/"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        tmp_url=<span class="string">"http://www.xiladaili.com/gaoni/&#123;&#125;/"</span></span><br><span class="line">        f = open(<span class="string">'proxies.txt'</span>, <span class="string">"r+"</span>)</span><br><span class="line">        f.truncate()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>,<span class="number">200</span>):<span class="comment">#数字可查看官网链接</span></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url=tmp_url.format(i),callback=self.parse)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self,response)</span>:</span></span><br><span class="line">        proxy_list=response.xpath(<span class="string">"//tbody//tr/td/text()"</span>).extract()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,len(proxy_list),<span class="number">8</span>):</span><br><span class="line">            self.verify_one_proxy(proxy_list[i+<span class="number">1</span>],proxy_list[i])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">verify_one_proxy</span><span class="params">(self,protocol,url)</span>:</span></span><br><span class="line">        schema = <span class="string">'https'</span> <span class="keyword">if</span> <span class="string">'https'</span> <span class="keyword">in</span> protocol <span class="keyword">else</span> <span class="string">'http'</span></span><br><span class="line">        proxies = &#123;schema: url&#125;</span><br><span class="line">        print(proxies)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">if</span> requests.get(<span class="string">'https://www.baidu.com/'</span>, proxies=proxies, timeout=<span class="number">2</span>).status_code == <span class="number">200</span>:</span><br><span class="line">                <span class="keyword">with</span> open(<span class="string">'./proxies.txt'</span>, <span class="string">'a+'</span>,encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">                    f.write(schema+<span class="string">"://"</span>+url+<span class="string">"\n"</span>)</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p>在 Scrapy 的 middlewares.py 中定义一个 ProxyMiddleWare 类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProxyMiddleWare</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""docstring for ProxyMiddleWare"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request, spider)</span>:</span></span><br><span class="line">        <span class="string">'''对request对象加上proxy'''</span></span><br><span class="line">        proxy = self.get_random_proxy()</span><br><span class="line">        print(<span class="string">"this is request ip:"</span> + proxy)</span><br><span class="line">        request.meta[<span class="string">'proxy'</span>] = proxy</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_response</span><span class="params">(self, request, response, spider)</span>:</span></span><br><span class="line">        <span class="string">'''对返回的response处理'''</span></span><br><span class="line">        <span class="comment"># 如果返回的response状态不是200,重新生成当前request对象</span></span><br><span class="line">        <span class="keyword">if</span> response.status != <span class="number">200</span>:</span><br><span class="line">            proxy = self.get_random_proxy()</span><br><span class="line">            print(<span class="string">"this is response ip:"</span> + proxy)</span><br><span class="line">            <span class="comment"># 对当前request加上代理</span></span><br><span class="line">            request.meta[<span class="string">'proxy'</span>] = proxy</span><br><span class="line">            <span class="keyword">return</span> request</span><br><span class="line">        <span class="keyword">return</span> response</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_random_proxy</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''随机从文件中读取proxy'''</span></span><br><span class="line">        <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">with</span> open(<span class="string">'./proxies.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">                proxies = f.readlines()</span><br><span class="line">            <span class="keyword">if</span> proxies:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line">                <span class="comment">#time.sleep(1)</span></span><br><span class="line">        proxy = random.choice(proxies).strip()</span><br><span class="line">        <span class="keyword">return</span> proxy</span><br></pre></td></tr></table></figure>
<p>在 setting.py 的 Download Middware 参数中添加配置的类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">    <span class="string">'crawlersec.middlewares.CrawlersecDownloaderMiddleware'</span>: <span class="number">543</span>,</span><br><span class="line">    <span class="string">'crawlersec.middlewares.ProxyMiddleWare'</span>: <span class="number">540</span>,</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="0x1-3-3-Cookie"><a href="#0x1-3-3-Cookie" class="headerlink" title="0x1.3.3 Cookie"></a>0x1.3.3 Cookie</h3><p>反爬策略：文章需要登录后才能访问<br>绕过方法：<br>1.手动登录获取 Cookie,将 Cookie 添加到爬虫脚本中<br>2.模拟登录 </p>
<h3 id="0x1-3-4-Header"><a href="#0x1-3-4-Header" class="headerlink" title="0x1.3.4 Header"></a>0x1.3.4 Header</h3><p>反爬策略：网站的文章需要带特定的头部请求才能允许访问,如Referer等<br>绕过方法：   每次请求中添加需要的头部<br>例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">headers=&#123;</span><br><span class="line">                <span class="string">"X-Requested-With"</span>:<span class="string">"XMLHttpRequest"</span>,</span><br><span class="line">                <span class="string">"Referer"</span>: <span class="string">"https://www.kanxue.com/"</span></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">yield</span> scrapy.FormRequest(url=url,formdata=data,headers=headers,callback=self.parse)</span><br></pre></td></tr></table></figure>

<h1 id="0x2-数据检索"><a href="#0x2-数据检索" class="headerlink" title="0x2.数据检索"></a>0x2.数据检索</h1><p>当数据爬取后，对数据建立倒排索引，方便我们快速搜索        </p>
<h2 id="0x2-1-倒排索引"><a href="#0x2-1-倒排索引" class="headerlink" title="0x2.1 倒排索引"></a>0x2.1 倒排索引</h2><p>倒排索引也称全文索引，检索程序对文章的每一个词建立一个索引，指明该词在文章中出现的次数和位置，当用户查询时，检索程序就根据事先建立的索引进行查找，并将查找的结果反馈给用户的检索方式。这个过程类似于通过字典中的检索字表查字的过程。</p>
<p>例如有两篇文章：<br>文章1内容：<code>it is sunny today</code><br>文章2内容：<code>today is rainy</code> </p>
<p>1.首先取得关键字<br>文章1关键字：<code>[it] [is] [sunny] [today]</code><br>文章2关键字：<code>[today] [is] [rainy]</code><br>2.建立倒排索引<br>有了关键词后，就可以建立倒排索引了。上面的对应关系是：“文章号”对“文章中所有关键词”。倒排索引把这个关系倒过来，变成: “关键词”对“拥有该关键词的所有文章号”。 </p>
<table>
<thead>
<tr>
<th>关键词</th>
<th>文章号</th>
</tr>
</thead>
<tbody><tr>
<td>it</td>
<td>1</td>
</tr>
<tr>
<td>is</td>
<td>1,2</td>
</tr>
<tr>
<td>sunny</td>
<td>1</td>
</tr>
<tr>
<td>today</td>
<td>1,2</td>
</tr>
<tr>
<td>rainy</td>
<td>2</td>
</tr>
</tbody></table>
<p>通常仅知道关键词在哪些文章中出现还不够，我们还需要知道关键词在文章中出现次数和出现的位置</p>
<table>
<thead>
<tr>
<th>关键词</th>
<th>文章号[出现频率]</th>
<th>出现位置</th>
</tr>
</thead>
<tbody><tr>
<td>it</td>
<td>1[1]</td>
<td>1</td>
</tr>
<tr>
<td>is</td>
<td>1[1]</td>
<td>2</td>
</tr>
<tr>
<td>is</td>
<td>2[1]</td>
<td>2</td>
</tr>
<tr>
<td>sunny</td>
<td>1[1]</td>
<td>3</td>
</tr>
<tr>
<td>today</td>
<td>1[1]</td>
<td>4</td>
</tr>
<tr>
<td>today</td>
<td>2[1]</td>
<td>1</td>
</tr>
<tr>
<td>rainy</td>
<td>2[1]</td>
<td>3</td>
</tr>
</tbody></table>
<p>实现时，将上面三列分别作为词典文件（Term Dictionary）、频率文件(frequencies)、位置文件 (positions)保存。其中词典文件不仅保存有每个关键词，还保留了指向频率文件和位置文件的指针，通过指针可以找到该关键字的频率信息和位置信息。    </p>
<h2 id="0x2-2-elasticsearch"><a href="#0x2-2-elasticsearch" class="headerlink" title="0x2.2 elasticsearch"></a>0x2.2 elasticsearch</h2><p>数据检索这里借助了 Elasticsearch。Elasticsearch的 Mapping 提供了对 Elasticsearch 中索引字段名及其数据类型的定义，还可以对某些字段添加特殊属性：该字段是否分词，是否存储，使用什么样的分词器等。</p>
<p>常用的数据类型(type)有：string、text、date等<br>Elaticsearch 的 mapping 样例如下，对文章链接、标题、作者、发布日期、正文内容、网站域名这六个字段指定检索方式   </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">mappings = &#123;</span><br><span class="line">        <span class="string">"mappings"</span>: &#123;</span><br><span class="line">                <span class="string">"properties"</span>: &#123;</span><br><span class="line">                    <span class="string">"url"</span>: &#123;</span><br><span class="line">                        <span class="string">"type"</span>: <span class="string">"keyword"</span></span><br><span class="line">                    &#125;,</span><br><span class="line">                    <span class="string">"title"</span>: &#123;</span><br><span class="line">                        <span class="string">"type"</span>: <span class="string">"text"</span>,</span><br><span class="line">                        <span class="string">"analyzer"</span>: <span class="string">"ik_max_word"</span>,</span><br><span class="line">                        <span class="string">"search_analyzer"</span>: <span class="string">"ik_max_word"</span>,</span><br><span class="line">                        <span class="string">"fields"</span>: &#123;</span><br><span class="line">                            <span class="string">"keyword"</span>: &#123;</span><br><span class="line">                                <span class="string">"type"</span>: <span class="string">"keyword"</span>,</span><br><span class="line">                                <span class="string">"ignore_above"</span>: <span class="number">256</span></span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;,</span><br><span class="line">                    <span class="string">"author"</span>: &#123;</span><br><span class="line">                        <span class="string">"type"</span>: <span class="string">"text"</span>,</span><br><span class="line">                        <span class="string">"analyzer"</span>: <span class="string">"ik_max_word"</span>,</span><br><span class="line">                        <span class="string">"search_analyzer"</span>: <span class="string">"ik_max_word"</span>,</span><br><span class="line">                        <span class="string">"fields"</span>: &#123;</span><br><span class="line">                            <span class="string">"keyword"</span>: &#123;</span><br><span class="line">                                <span class="string">"type"</span>: <span class="string">"keyword"</span>,</span><br><span class="line">                                <span class="string">"ignore_above"</span>: <span class="number">256</span></span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;,</span><br><span class="line">                    <span class="string">"date"</span>: &#123;</span><br><span class="line">                        <span class="string">"type"</span>: <span class="string">"date"</span>,</span><br><span class="line">                        <span class="string">"format"</span>: <span class="string">"yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis"</span></span><br><span class="line"></span><br><span class="line">                    &#125;,</span><br><span class="line">                    <span class="string">"content"</span>: &#123;</span><br><span class="line">                        <span class="string">"type"</span>: <span class="string">"text"</span>,</span><br><span class="line">                        <span class="string">"analyzer"</span>: <span class="string">"ik_max_word"</span>,</span><br><span class="line">                        <span class="string">"search_analyzer"</span>: <span class="string">"ik_max_word"</span></span><br><span class="line">                    &#125;,</span><br><span class="line">                    <span class="string">"domain"</span>: &#123;</span><br><span class="line">                        <span class="string">"type"</span>: <span class="string">"text"</span>,</span><br><span class="line">                        <span class="string">"analyzer"</span>: <span class="string">"ik_max_word"</span>,</span><br><span class="line">                        <span class="string">"search_analyzer"</span>: <span class="string">"ik_max_word"</span>,</span><br><span class="line">                        <span class="string">"fields"</span>: &#123;</span><br><span class="line">                            <span class="string">"keyword"</span>: &#123;</span><br><span class="line">                                <span class="string">"type"</span>: <span class="string">"keyword"</span>,</span><br><span class="line">                                <span class="string">"ignore_above"</span>: <span class="number">256</span></span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h1 id="0x3-数据搜索"><a href="#0x3-数据搜索" class="headerlink" title="0x3.数据搜索"></a>0x3.数据搜索</h1><p>对数据检索完后，就可以搜索，然而如果搜索结果有一千个，甚至成千上万个呢？哪个又是您最想要的文章？</p>
<p>打开 google ,搜索 “web安全”，返回 518000000 条结果，好大的一个数字，在众多的搜索结果中，如何将最相关的放在最前面？<br><img src="../../images/scrapy/secarticle/7.png" alt=""></p>
<p>先简单了解一下数据搜索的过程：<br>1.搜索字符串分词<br>对输入的 “web安全”进行分词：web 、安全、安、全<br>2.搜索字符串和文档的相关性计算<br>首先，一个文档有很多词(Term)组成，如web、安全、安、全、等。<br>其次对于文档之间的关系，不同的 Term 重要性不同，比如对于本篇文档，“web、安全”就相对重要一些，“的、地、可”可能相对不重要一些。所以如果两篇文档都包含“web、安全”，这两篇文档的相关性好一些，然而就算一篇文档包含“的、地、可”，另一篇文档不包含“的、地、可”，也不能影响两篇文档的相关性。</p>
<p>因而判断文档之间的关系，首先找出哪些词(Term)对文档之间的关系最重要，如“web、安全”,然后判断这些词(Term)之间的关系。  </p>
<p><strong>找出词(Term)对文档的重要性的过程称为计算词的权重(Term weight)的过程。</strong><br>计算词的权重(Term weight)有两个参数，第一个是词(Term)，第二个是文档(Document)<br><strong>判断词(Term)之间的关系从而得到文档相关性的过程应用一种叫做向量空间模型的算法(Vector Space Model)。</strong> </p>
<p>（1）计算权重<br>影响一个词(Term)在一篇文档中的重要性主要有两个因素：</p>
<ul>
<li>Term Frequency (tf)：即此Term在此文档中出现了多少次。tf 越大说明越重要。</li>
<li>Document Frequency (df)：即有多少文档包含次Term。df 越大说明越不重要。  </li>
</ul>
<p>词(Term)在文档中出现的次数越多，说明此词(Term)对该文档越重要，如“web”这个词，在本文档中出现的次数很多，说明本文档主要就是讲这方面的事的。然而在一篇文档中，“的”出现的次数更多，就说明越重要吗？不是的，这是由第二个因素进行调整，第二个因素说明，有越多的文档包含此词(Term), 说明此词(Term)太普通，不足以区分这些文档，因而重要性越低。<br>权重计算公式如下：<br><img src="../../images/scrapy/secarticle/8.png" alt=""><br><img src="../../images/scrapy/secarticle/9.png" alt=""><br>（2）向量空间模型的算法(VSM)<br>我们把文档看作一系列词(Term)，每一个词(Term)都有一个权重(Term weight)，不同的词(Term)根据自己在文档中的权重来影响文档相关性的打分计算。<br>于是我们把所有此文档中词(term)的权重(term weight) 看作一个向量。<br>Document = {term1, term2, …… ,term N}<br>Document Vector = {weight1, weight2, …… ,weight N}<br>同样我们把查询语句看作一个简单的文档，也用向量来表示。<br>Query = {term1, term 2, …… , term N}<br>Query Vector = {weight1, weight2, …… , weight N}<br>我们把所有搜索出的文档向量及查询向量放到一个N维空间中，每个词(term)是一维。</p>
<p>如图：<br><img src="../../images/scrapy/secarticle/10.jpg" alt=""></p>
<p>我们认为两个向量之间的夹角越小，相关性越大。</p>
<p>所以我们计算夹角的余弦值作为相关性的打分，夹角越小，余弦值越大，打分越高，相关性越大。<br>相关性打分公式如下：<br><img src="../../images/scrapy/secarticle/11.png" alt=""></p>
<h1 id="0x4-系统展示"><a href="#0x4-系统展示" class="headerlink" title="0x4 系统展示"></a>0x4 系统展示</h1><p>最终项目地址：<a href="http://secsea.cfyqy.com/" target="_blank" rel="noopener">http://secsea.cfyqy.com/</a>  。写得有点简洁，莫喷。<br>1.web界面<br>显示最近一周的实时文章和相关资讯<br><img src="../../images/scrapy/secarticle/12.png" alt=""><br>显示收录文章数量比较多的站点<br><img src="../../images/scrapy/secarticle/13.png" alt=""><br>2.搜索功能<br>提供了正文内容搜索、标题搜索、作者搜索、时间搜索、站点数据搜索功能。</p>
<p>默认使用的是正文内容搜索<br><img src="../../images/scrapy/secarticle/14.png" alt=""><br>只想要某个站点的数据，并显示最近一年的<br><img src="../../images/scrapy/secarticle/15.png" alt=""> </p>
<p>参考文章：<br>倒排索引原理和实现: <a href="https://www.cnblogs.com/binyue/p/3380750.html" target="_blank" rel="noopener">https://www.cnblogs.com/binyue/p/3380750.html</a><br>全文检索的基本原理：<a href="https://www.cnblogs.com/forfuture1978/archive/2009/12/14/1623594.html" target="_blank" rel="noopener">https://www.cnblogs.com/forfuture1978/archive/2009/12/14/1623594.html</a></p>
</div><div class="post-copyright"><div class="post-copyright-author"><span class="post-copyright-meta">本文作者: </span><span class="post-copyright-info"><a href="mailto:undefined">ye1s</a></span></div><div class="post-copyright-type"><span class="post-copyright-meta">本文链接: </span><span class="post-copyright-info"><a href="https://blog.cfyqy.com/article/f76a9d38.html">https://blog.cfyqy.com/article/f76a9d38.html</a></span></div><div class="post-copyright-notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://blog.cfyqy.com">ye1s</a>！</span></div></div><div class="post-copyright" id="comments-container"><script src="/js/comments/gitment.js"></script></div><script>let arr = location.href.split('/#more')[0].split('/');
let title = arr[arr.length - 1];
if (title === '') {
    title = arr[arr.length - 2]
}
var flag = false;
var gitFun = function () {
    try {
        var gitmentObj = window.GLOBAL_CONFIG.gitment;
        var gitment = new Gitment({
            id: decodeURI(title), // 可选。默认为 location.href
            owner: gitmentObj.owner,
            repo: gitmentObj.repo,
            oauth: {
                client_id: gitmentObj.client_id,
                client_secret: gitmentObj.client_secret
            },
        });
        gitment.render('comments-container');
        flag = true;
    } catch (e) {
        flag = false;
    }
}
var setIn = setInterval(() => {
    if (!flag) {
        gitFun();
    } else {
        clearInterval(setIn);
    }
}, 200);</script></article><div id="pagination"><div class="prev-post pull-left"><span class="line line-top"></span><span class="line line-right"></span><span class="line line-bottom"></span><span class="line line-left"></span><a href="/article/88d97774.html"><i class="fas fa-angle-left">&nbsp;</i><span>sql盲注效率分析</span></a></div><div class="next-post pull-right"><span class="line line-top"></span><span class="line line-right"></span><span class="line line-bottom"></span><span class="line line-left"></span><a href="/article/64465197.html"><span>AWD攻防技巧</span><span>&nbsp;</span><i class="fas fa-angle-right"></i></a></div></div><!--div!= paginator()--></div></div><div class="button-hover" id="return-top"><i class="fas fa-arrow-up" aria-hidden="true"></i></div><footer><div id="footer"><div class="button-hover" id="side-button"><i class="fas fa-arrow-right"></i></div><div class="right-content"><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fas fa-file-o"></i></span><span id="busuanzi_value_page_pv"></span><span></span></div><div class="copyright">&copy;2017 ～ 2021 By ye1s</div></div></div></footer></div><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery-3.3.1.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/velocity.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery.mCustomScrollbar.concat.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/fan.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/canvas_bg.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/utils.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/scroll.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/sidebar.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/copy.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/daovoice.js"></script><!--script(src=url)--><div class="search-dialog"><div id="algolia-search-title">Algolia</div><div class="search-close-button"><i class="fa fa-times"></i></div><!--div#current-refined-values--><!--div#clear-all--><div id="search-box"></div><!--div#refinement-list--><hr><div id="hits"></div><div id="algolia-pagination"></div></div><div class="search-mask"></div><script src="/js/search/algolia.js"></script></body></html>